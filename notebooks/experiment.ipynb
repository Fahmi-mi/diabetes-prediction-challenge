{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Experimentation\n",
        "# Only run once\n",
        "import sys\n",
        "import os\n",
        "\n",
        "project_root = os.path.abspath('../')\n",
        "os.chdir(project_root)\n",
        "\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22302dea",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8b3a8d7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from utils.load_data import DataLoader\n",
        "from utils.preprocess import Preprocessor, FeatureEngineering\n",
        "from utils.evaluate_model import Evaluator\n",
        "from utils.model import ModelLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "import mlflow\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "dl = DataLoader()\n",
        "pre = Preprocessor()\n",
        "fe = FeatureEngineering()\n",
        "evaluator = Evaluator()\n",
        "ml = ModelLoader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a73fa3e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"file:///c:/Users/Axioo/Documents/Fahmi/ai/ml/diabetes-prediction-challenge/logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "be57e019",
      "metadata": {},
      "outputs": [],
      "source": [
        "train = dl.load_data('train_clean.csv', 'data/processed')\n",
        "test = dl.load_data('test_clean.csv', 'data/processed')\n",
        "submission = dl.load_data('sample_submission.csv', 'data/raw')\n",
        "\n",
        "train_df = train.copy()\n",
        "test_df = test.copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = dl.split_data(train_df, id_column='id', target_column='diagnosed_diabetes', test_size=0.2, random_state=42, stratify='diagnosed_diabetes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47d745c",
      "metadata": {},
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8cab7b32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function\n",
        "def create_features(df):\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # 1. BMI Categories\n",
        "    df_new['bmi_category'] = pd.cut(df_new['bmi'], bins=[0, 18.5, 25, 30, 100],labels=['underweight', 'normal', 'overweight', 'obese'])\n",
        "    \n",
        "    # 2. Age Groups\n",
        "    df_new['age_group'] = pd.cut(df_new['age'], bins=[0, 30, 45, 60, 100],labels=['young', 'middle', 'senior', 'elderly'])\n",
        "    \n",
        "    # 3. Blood Pressure Categories (using systolic_bp)\n",
        "    df_new['bp_category'] = pd.cut(df_new['systolic_bp'],bins=[0, 120, 130, 140, 200],labels=['normal', 'elevated', 'high', 'very_high'])\n",
        "    \n",
        "    # 4. Cardiovascular Risk Score\n",
        "    df_new['cardio_risk_score'] = (\n",
        "        (df_new['systolic_bp'] / 140) * 0.3 +\n",
        "        (df_new['diastolic_bp'] / 90) * 0.3 +\n",
        "        (df_new['heart_rate'] / 100) * 0.2 +\n",
        "        df_new['cardiovascular_history'] * 0.2\n",
        "    )\n",
        "    \n",
        "    # 5. Cholesterol Risk Score\n",
        "    df_new['cholesterol_risk'] = (\n",
        "        (df_new['cholesterol_total'] / 240) * 0.4 +\n",
        "        (df_new['ldl_cholesterol'] / 160) * 0.4 +\n",
        "        (1 - df_new['hdl_cholesterol'] / 60) * 0.2\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    # 6. Lifestyle Risk Score\n",
        "    df_new['lifestyle_risk'] = (\n",
        "        (df_new['alcohol_consumption_per_week'] / 7) * 0.2 +\n",
        "        (1 - df_new['physical_activity_minutes_per_week'] / 300) * 0.3 +\n",
        "        (df_new['screen_time_hours_per_day'] / 10) * 0.2 +\n",
        "        (1 - df_new['sleep_hours_per_day'] / 8) * 0.3\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    # 7. Diet-Activity Balance\n",
        "    df_new['diet_activity_balance'] = (\n",
        "        df_new['diet_score'] * df_new['physical_activity_minutes_per_week'] / 1000\n",
        "    )\n",
        "    \n",
        "    # 8. Metabolic Health Index\n",
        "    df_new['metabolic_index'] = (\n",
        "        (df_new['triglycerides'] / 150) * 0.3 +\n",
        "        (df_new['bmi'] / 30) * 0.4 +\n",
        "        (df_new['waist_to_hip_ratio'] / 0.95) * 0.3\n",
        "    )\n",
        "    \n",
        "    # 9. Overall Health Risk\n",
        "    df_new['overall_health_risk'] = (\n",
        "        df_new['cardio_risk_score'] * 0.25 +\n",
        "        df_new['cholesterol_risk'] * 0.25 +\n",
        "        df_new['lifestyle_risk'] * 0.2 +\n",
        "        df_new['metabolic_index'] * 0.3\n",
        "    )\n",
        "    \n",
        "    # 10. Interaction Features\n",
        "    df_new['age_bmi'] = df_new['age'] * df_new['bmi']\n",
        "    df_new['age_bp'] = df_new['age'] * df_new['systolic_bp']\n",
        "    df_new['bmi_cholesterol'] = df_new['bmi'] * df_new['cholesterol_total']\n",
        "    df_new['family_history_age'] = df_new['family_history_diabetes'] * df_new['age']\n",
        "    \n",
        "    # 11. Log Transform for Skewed Features\n",
        "    df_new['log_physical_activity'] = np.log1p(df_new['physical_activity_minutes_per_week'])\n",
        "    df_new['log_triglycerides'] = np.log1p(df_new['triglycerides'])\n",
        "    \n",
        "    # 12. Squared Features for Non-linear Relationships\n",
        "    df_new['age_squared'] = df_new['age'] ** 2\n",
        "    df_new['bmi_squared'] = df_new['bmi'] ** 2\n",
        "    \n",
        "    # 13. Binary Risk Flags\n",
        "    df_new['high_bp_flag'] = (df_new['systolic_bp'] > 140).astype(int)\n",
        "    df_new['high_cholesterol_flag'] = (df_new['cholesterol_total'] > 240).astype(int)\n",
        "    df_new['obese_flag'] = (df_new['bmi'] > 30).astype(int)\n",
        "    df_new['sedentary_flag'] = (df_new['physical_activity_minutes_per_week'] < 150).astype(int)\n",
        "    \n",
        "    # 14. Combined Family and Medical History\n",
        "    df_new['medical_history_score'] = (\n",
        "        df_new['family_history_diabetes'] + \n",
        "        df_new['hypertension_history'] + \n",
        "        df_new['cardiovascular_history']\n",
        "    )\n",
        "    \n",
        "    return df_new\n",
        "\n",
        "def prepare_features(df, categorical_cols, is_train=True, train_columns=None):\n",
        "    df_fe = create_features(df)\n",
        "    cat_cols = categorical_cols + ['bmi_category', 'age_group', 'bp_category']\n",
        "    df_encoded = pd.get_dummies(df_fe, columns=cat_cols, drop_first=True)\n",
        "    \n",
        "    if is_train:\n",
        "        return df_encoded, df_encoded.columns.tolist()\n",
        "    else:\n",
        "        if train_columns is None:\n",
        "            raise ValueError(\"train_columns must be provided when is_train=False\")\n",
        "        \n",
        "        for col in train_columns:\n",
        "            if col not in df_encoded.columns:\n",
        "                df_encoded[col] = 0\n",
        "        \n",
        "        df_encoded = df_encoded[train_columns]\n",
        "        \n",
        "        return df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8beca1bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execution\n",
        "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "X_train_fe, train_columns = prepare_features(X_train, categorical_cols, is_train=True)\n",
        "X_val_fe = prepare_features(X_val, categorical_cols, is_train=False, train_columns=train_columns)\n",
        "test_fe = prepare_features(test_df, categorical_cols, is_train=False, train_columns=train_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65898c41",
      "metadata": {},
      "source": [
        "# Tuning hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0a48f011",
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "for col in X_train_fe.select_dtypes(include='uint8').columns:\n",
        "    X_train_fe[col] = X_train_fe[col].astype('category')\n",
        "    X_val_fe[col] = X_val_fe[col].astype('category')\n",
        "        \n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"random_state\": 42,\n",
        "        \"verbosity\": -1,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 50, 300),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 15),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10, log=True),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-3, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-3, 10.0, log=True),\n",
        "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.0, 1.0),\n",
        "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 5.0),\n",
        "    }\n",
        "    \n",
        "    dtrain = lgb.Dataset(X_train_fe, label=y_train)\n",
        "    dval = lgb.Dataset(X_val_fe, label=y_val, reference=dtrain)\n",
        "    \n",
        "    with mlflow.start_run(nested=True):\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dval],\n",
        "            num_boost_round=500,\n",
        "            callbacks=[lgb.early_stopping(100)]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val_fe, num_iteration=model.best_iteration)\n",
        "        auc = roc_auc_score(y_val, preds)\n",
        "\n",
        "        mlflow.log_params(params)\n",
        "        mlflow.log_metric(\"val_auc\", auc)\n",
        "        mlflow.log_metric(\"best_iteration\", model.best_iteration)\n",
        "        mlflow.log_metric(\"train_auc\", roc_auc_score(y_train, model.predict(X_train_fe, num_iteration=model.best_iteration)))\n",
        "        \n",
        "        signature = mlflow.models.infer_signature(X_train_fe, preds)\n",
        "        mlflow.lightgbm.log_model(\n",
        "            model, \n",
        "            name=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=None\n",
        "        )\n",
        "\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ab17d9be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's binary_logloss: 0.650221\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.632126\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.661875\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.66183\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's binary_logloss: 0.639075\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's binary_logloss: 0.657096\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's binary_logloss: 0.641006\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\tvalid_0's binary_logloss: 0.599365\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's binary_logloss: 0.65855\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's binary_logloss: 0.643595\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.585984\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.587039\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.58627\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[490]\tvalid_0's binary_logloss: 0.603599\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.611282\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's binary_logloss: 0.650616\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.607721\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.585144\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's binary_logloss: 0.656639\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[154]\tvalid_0's binary_logloss: 0.624057\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.657909\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.592247\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.595883\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[490]\tvalid_0's binary_logloss: 0.59661\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[38]\tvalid_0's binary_logloss: 0.627574\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's binary_logloss: 0.646445\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[460]\tvalid_0's binary_logloss: 0.598049\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's binary_logloss: 0.63611\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[463]\tvalid_0's binary_logloss: 0.604121\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's binary_logloss: 0.644988\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.64976\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[455]\tvalid_0's binary_logloss: 0.597851\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's binary_logloss: 0.631595\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[446]\tvalid_0's binary_logloss: 0.593262\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.616425\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.593433\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.631262\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[404]\tvalid_0's binary_logloss: 0.608268\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's binary_logloss: 0.640371\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's binary_logloss: 0.653261\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.59034\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[458]\tvalid_0's binary_logloss: 0.597683\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[488]\tvalid_0's binary_logloss: 0.601295\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[499]\tvalid_0's binary_logloss: 0.58446\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[40]\tvalid_0's binary_logloss: 0.627242\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[493]\tvalid_0's binary_logloss: 0.60548\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.635702\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.58795\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[489]\tvalid_0's binary_logloss: 0.588103\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's binary_logloss: 0.626207\n",
            "Best AUC: 0.725471\n",
            "Best params:\n",
            "  learning_rate: 0.03193479514950249\n",
            "  num_leaves: 195\n",
            "  max_depth: 12\n",
            "  min_child_samples: 89\n",
            "  min_child_weight: 0.002670957200426603\n",
            "  feature_fraction: 0.6427016380380681\n",
            "  bagging_fraction: 0.7767137491282269\n",
            "  bagging_freq: 7\n",
            "  lambda_l1: 4.66550645007372\n",
            "  lambda_l2: 0.14447176871483172\n",
            "  min_gain_to_split: 0.9915928970320315\n",
            "  scale_pos_weight: 1.4348046535341124\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_experiment(\"lgbm\")\n",
        "with mlflow.start_run(run_name=\"lgbm_hyperparameter_tuning_1\"):\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=50)\n",
        "    \n",
        "    best_params = study.best_params\n",
        "    best_value = study.best_value\n",
        "    \n",
        "    print(f\"Best AUC: {study.best_value:.6f}\")\n",
        "    print(\"Best params:\")\n",
        "    for k, v in study.best_params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    \n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"best_val_auc\", study.best_value)\n",
        "    mlflow.log_metric(\"n_trials_completed\", len(study.trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7027b2bd",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8f71c3ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to models\\lgbm_model_1.pkl\n"
          ]
        }
      ],
      "source": [
        "final_params = best_params.copy()\n",
        "final_params.update({\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"random_state\": 42,\n",
        "    \"verbosity\": -1,\n",
        "})\n",
        "\n",
        "full_train_df = pd.concat([X_train, X_val], axis=0)\n",
        "full_y = pd.concat([y_train, y_val], axis=0)\n",
        "\n",
        "full_train_fe, full_train_columns = prepare_features(full_train_df, categorical_cols, is_train=True)\n",
        "\n",
        "# Set kategori jika perlu\n",
        "for col in categorical_cols:\n",
        "    if col in full_train_fe.columns:\n",
        "        full_train_fe[col] = full_train_fe[col].astype('category')\n",
        "\n",
        "# Training final model\n",
        "dtrain_full = lgb.Dataset(full_train_fe, label=full_y)\n",
        "final_model = lgb.train(\n",
        "    final_params,\n",
        "    dtrain_full,\n",
        "    num_boost_round=1000,\n",
        ")\n",
        "\n",
        "ml.save_model(final_model, \"lgbm_model_1.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703fe37d",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1633c5fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models\\lgbm_model_1.pkl\n"
          ]
        }
      ],
      "source": [
        "model = ml.load_model(\"lgbm_model_1.pkl\")\n",
        "preds = model.predict(test_fe)\n",
        "pred_labels = (preds > 0.5).astype(int)\n",
        "\n",
        "submission['diagnosed_diabetes'] = preds\n",
        "submission.to_csv('output/submission_1.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
