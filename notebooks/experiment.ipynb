{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Experimentation\n",
        "# Only run once\n",
        "import sys\n",
        "import os\n",
        "\n",
        "project_root = os.path.abspath('../')\n",
        "os.chdir(project_root)\n",
        "\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22302dea",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8b3a8d7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from utils.load_data import DataLoader\n",
        "from utils.preprocess import Preprocessor, FeatureEngineering\n",
        "from utils.evaluate_model import Evaluator\n",
        "from utils.model import ModelLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "import mlflow\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "dl = DataLoader()\n",
        "pre = Preprocessor()\n",
        "fe = FeatureEngineering()\n",
        "evaluator = Evaluator()\n",
        "ml = ModelLoader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a73fa3e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"file:///c:/Users/Axioo/Documents/Fahmi/ai/ml/diabetes-prediction-challenge/logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "be57e019",
      "metadata": {},
      "outputs": [],
      "source": [
        "train = dl.load_data('train_clean.csv', 'data/processed')\n",
        "test = dl.load_data('test_clean.csv', 'data/processed')\n",
        "submission = dl.load_data('sample_submission.csv', 'data/raw')\n",
        "\n",
        "train_df = train.copy()\n",
        "test_df = test.copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = dl.split_data(train_df, id_column='id', target_column='diagnosed_diabetes', test_size=0.2, random_state=42, stratify='diagnosed_diabetes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47d745c",
      "metadata": {},
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8cab7b32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function\n",
        "def create_features(df):\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # 1. BMI Categories\n",
        "    df_new['bmi_category'] = pd.cut(df_new['bmi'], bins=[0, 18.5, 25, 30, 100],labels=['underweight', 'normal', 'overweight', 'obese'])\n",
        "    \n",
        "    # 2. Age Groups\n",
        "    df_new['age_group'] = pd.cut(df_new['age'], bins=[0, 30, 45, 60, 100],labels=['young', 'middle', 'senior', 'elderly'])\n",
        "    \n",
        "    # 3. Blood Pressure Categories (using systolic_bp)\n",
        "    df_new['bp_category'] = pd.cut(df_new['systolic_bp'],bins=[0, 120, 130, 140, 200],labels=['normal', 'elevated', 'high', 'very_high'])\n",
        "    \n",
        "    # 4. Cardiovascular Risk Score\n",
        "    df_new['cardio_risk_score'] = (\n",
        "        (df_new['systolic_bp'] / 140) * 0.3 +\n",
        "        (df_new['diastolic_bp'] / 90) * 0.3 +\n",
        "        (df_new['heart_rate'] / 100) * 0.2 +\n",
        "        df_new['cardiovascular_history'] * 0.2\n",
        "    )\n",
        "    \n",
        "    # 5. Cholesterol Risk Score\n",
        "    df_new['cholesterol_risk'] = (\n",
        "        (df_new['cholesterol_total'] / 240) * 0.4 +\n",
        "        (df_new['ldl_cholesterol'] / 160) * 0.4 +\n",
        "        (1 - df_new['hdl_cholesterol'] / 60) * 0.2\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    # 6. Lifestyle Risk Score\n",
        "    df_new['lifestyle_risk'] = (\n",
        "        (df_new['alcohol_consumption_per_week'] / 7) * 0.2 +\n",
        "        (1 - df_new['physical_activity_minutes_per_week'] / 300) * 0.3 +\n",
        "        (df_new['screen_time_hours_per_day'] / 10) * 0.2 +\n",
        "        (1 - df_new['sleep_hours_per_day'] / 8) * 0.3\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    # 7. Diet-Activity Balance\n",
        "    df_new['diet_activity_balance'] = (\n",
        "        df_new['diet_score'] * df_new['physical_activity_minutes_per_week'] / 1000\n",
        "    )\n",
        "    \n",
        "    # 8. Metabolic Health Index\n",
        "    df_new['metabolic_index'] = (\n",
        "        (df_new['triglycerides'] / 150) * 0.3 +\n",
        "        (df_new['bmi'] / 30) * 0.4 +\n",
        "        (df_new['waist_to_hip_ratio'] / 0.95) * 0.3\n",
        "    )\n",
        "    \n",
        "    # 9. Overall Health Risk\n",
        "    df_new['overall_health_risk'] = (\n",
        "        df_new['cardio_risk_score'] * 0.25 +\n",
        "        df_new['cholesterol_risk'] * 0.25 +\n",
        "        df_new['lifestyle_risk'] * 0.2 +\n",
        "        df_new['metabolic_index'] * 0.3\n",
        "    )\n",
        "    \n",
        "    # 10. Interaction Features\n",
        "    df_new['age_bmi'] = df_new['age'] * df_new['bmi']\n",
        "    df_new['age_bp'] = df_new['age'] * df_new['systolic_bp']\n",
        "    df_new['bmi_cholesterol'] = df_new['bmi'] * df_new['cholesterol_total']\n",
        "    df_new['family_history_age'] = df_new['family_history_diabetes'] * df_new['age']\n",
        "    \n",
        "    # 11. Log Transform for Skewed Features\n",
        "    df_new['log_physical_activity'] = np.log1p(df_new['physical_activity_minutes_per_week'])\n",
        "    df_new['log_triglycerides'] = np.log1p(df_new['triglycerides'])\n",
        "    \n",
        "    # 12. Squared Features for Non-linear Relationships\n",
        "    df_new['age_squared'] = df_new['age'] ** 2\n",
        "    df_new['bmi_squared'] = df_new['bmi'] ** 2\n",
        "    \n",
        "    # 13. Binary Risk Flags\n",
        "    df_new['high_bp_flag'] = (df_new['systolic_bp'] > 140).astype(int)\n",
        "    df_new['high_cholesterol_flag'] = (df_new['cholesterol_total'] > 240).astype(int)\n",
        "    df_new['obese_flag'] = (df_new['bmi'] > 30).astype(int)\n",
        "    df_new['sedentary_flag'] = (df_new['physical_activity_minutes_per_week'] < 150).astype(int)\n",
        "    \n",
        "    # 14. Combined Family and Medical History\n",
        "    df_new['medical_history_score'] = (\n",
        "        df_new['family_history_diabetes'] + \n",
        "        df_new['hypertension_history'] + \n",
        "        df_new['cardiovascular_history']\n",
        "    )\n",
        "    \n",
        "    return df_new\n",
        "\n",
        "def prepare_features(df, categorical_cols, is_train=True, train_columns=None):\n",
        "    df_fe = create_features(df)\n",
        "    cat_cols = categorical_cols + ['bmi_category', 'age_group', 'bp_category']\n",
        "    df_encoded = pd.get_dummies(df_fe, columns=cat_cols, drop_first=True)\n",
        "    \n",
        "    if is_train:\n",
        "        return df_encoded, df_encoded.columns.tolist()\n",
        "    else:\n",
        "        if train_columns is None:\n",
        "            raise ValueError(\"train_columns must be provided when is_train=False\")\n",
        "        \n",
        "        for col in train_columns:\n",
        "            if col not in df_encoded.columns:\n",
        "                df_encoded[col] = 0\n",
        "        \n",
        "        df_encoded = df_encoded[train_columns]\n",
        "        \n",
        "        return df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8beca1bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execution\n",
        "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "X_train_fe, train_columns = prepare_features(X_train, categorical_cols, is_train=True)\n",
        "X_val_fe = prepare_features(X_val, categorical_cols, is_train=False, train_columns=train_columns)\n",
        "test_fe = prepare_features(test_df, categorical_cols, is_train=False, train_columns=train_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65898c41",
      "metadata": {},
      "source": [
        "# Tuning hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0a48f011",
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "for col in X_train_fe.select_dtypes(include='uint8').columns:\n",
        "    X_train_fe[col] = X_train_fe[col].astype('category')\n",
        "    X_val_fe[col] = X_val_fe[col].astype('category')\n",
        "        \n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"random_state\": 42,\n",
        "        \"verbosity\": -1,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 50, 300),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 15),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10, log=True),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-3, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-3, 10.0, log=True),\n",
        "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.0, 1.0),\n",
        "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 5.0),\n",
        "    }\n",
        "    \n",
        "    dtrain = lgb.Dataset(X_train_fe, label=y_train)\n",
        "    dval = lgb.Dataset(X_val_fe, label=y_val, reference=dtrain)\n",
        "    \n",
        "    with mlflow.start_run(nested=True):\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dval],\n",
        "            num_boost_round=700,\n",
        "            callbacks=[lgb.early_stopping(100)]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val_fe, num_iteration=model.best_iteration)\n",
        "        auc = roc_auc_score(y_val, preds)\n",
        "\n",
        "        mlflow.log_params(params)\n",
        "        mlflow.log_metric(\"val_auc\", auc)\n",
        "        mlflow.log_metric(\"best_iteration\", model.best_iteration)\n",
        "        mlflow.log_metric(\"train_auc\", roc_auc_score(y_train, model.predict(X_train_fe, num_iteration=model.best_iteration)))\n",
        "        \n",
        "        signature = mlflow.models.infer_signature(X_train_fe, preds)\n",
        "        mlflow.lightgbm.log_model(\n",
        "            model, \n",
        "            name=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=None\n",
        "        )\n",
        "\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ab17d9be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's binary_logloss: 0.654944\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's binary_logloss: 0.659636\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.658341\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\tvalid_0's binary_logloss: 0.650045\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[80]\tvalid_0's binary_logloss: 0.637346\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[694]\tvalid_0's binary_logloss: 0.613037\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[312]\tvalid_0's binary_logloss: 0.590181\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's binary_logloss: 0.652618\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.661039\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.653757\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.584713\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.584944\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.58488\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's binary_logloss: 0.625215\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.603248\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's binary_logloss: 0.640714\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[698]\tvalid_0's binary_logloss: 0.584394\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's binary_logloss: 0.636717\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[692]\tvalid_0's binary_logloss: 0.597486\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.60898\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's binary_logloss: 0.634468\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[697]\tvalid_0's binary_logloss: 0.595216\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[696]\tvalid_0's binary_logloss: 0.591348\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[626]\tvalid_0's binary_logloss: 0.584357\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's binary_logloss: 0.635112\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[692]\tvalid_0's binary_logloss: 0.599345\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\tvalid_0's binary_logloss: 0.642201\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.593154\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[696]\tvalid_0's binary_logloss: 0.609744\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's binary_logloss: 0.654987\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's binary_logloss: 0.64481\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.611575\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[684]\tvalid_0's binary_logloss: 0.612701\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[39]\tvalid_0's binary_logloss: 0.627875\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.604807\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's binary_logloss: 0.657758\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's binary_logloss: 0.64316\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[695]\tvalid_0's binary_logloss: 0.609946\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's binary_logloss: 0.649762\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's binary_logloss: 0.635068\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.612029\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.614452\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.607065\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.606376\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[672]\tvalid_0's binary_logloss: 0.58736\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.634728\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.585832\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[662]\tvalid_0's binary_logloss: 0.586249\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[694]\tvalid_0's binary_logloss: 0.586602\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[696]\tvalid_0's binary_logloss: 0.588172\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[538]\tvalid_0's binary_logloss: 0.586168\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[679]\tvalid_0's binary_logloss: 0.586544\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[452]\tvalid_0's binary_logloss: 0.58601\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.585066\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[690]\tvalid_0's binary_logloss: 0.585749\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.594295\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[684]\tvalid_0's binary_logloss: 0.595766\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.592261\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.656962\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[663]\tvalid_0's binary_logloss: 0.592635\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[694]\tvalid_0's binary_logloss: 0.599666\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[661]\tvalid_0's binary_logloss: 0.583723\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.583901\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[695]\tvalid_0's binary_logloss: 0.583921\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[396]\tvalid_0's binary_logloss: 0.592063\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.590022\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[698]\tvalid_0's binary_logloss: 0.597059\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's binary_logloss: 0.657208\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[493]\tvalid_0's binary_logloss: 0.589754\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.601824\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[695]\tvalid_0's binary_logloss: 0.619885\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[696]\tvalid_0's binary_logloss: 0.583585\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[654]\tvalid_0's binary_logloss: 0.592948\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[630]\tvalid_0's binary_logloss: 0.583434\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[698]\tvalid_0's binary_logloss: 0.590228\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.595513\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[504]\tvalid_0's binary_logloss: 0.594543\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.601109\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.621085\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.656544\n",
            "Best AUC: 0.727090\n",
            "Best params:\n",
            "  learning_rate: 0.0660608829830811\n",
            "  num_leaves: 97\n",
            "  max_depth: 8\n",
            "  min_child_samples: 100\n",
            "  min_child_weight: 1.2688200525504028\n",
            "  feature_fraction: 0.8136039969314071\n",
            "  bagging_fraction: 0.9672504611635335\n",
            "  bagging_freq: 1\n",
            "  lambda_l1: 8.248856018265785\n",
            "  lambda_l2: 6.595500956276427\n",
            "  min_gain_to_split: 0.9267058654853447\n",
            "  scale_pos_weight: 1.3247114020872859\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_experiment(\"lgbm\")\n",
        "with mlflow.start_run(run_name=\"lgbm_hyperparameter_tuning_2\"):\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=80)\n",
        "    \n",
        "    best_params = study.best_params\n",
        "    best_value = study.best_value\n",
        "    \n",
        "    print(f\"Best AUC: {study.best_value:.6f}\")\n",
        "    print(\"Best params:\")\n",
        "    for k, v in study.best_params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    \n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"best_val_auc\", study.best_value)\n",
        "    mlflow.log_metric(\"n_trials_completed\", len(study.trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7027b2bd",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f71c3ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to models\\lgbm_model_2.pkl\n"
          ]
        }
      ],
      "source": [
        "final_params = best_params.copy()\n",
        "final_params.update({\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"random_state\": 42,\n",
        "    \"verbosity\": -1,\n",
        "})\n",
        "\n",
        "full_train_df = pd.concat([X_train, X_val], axis=0)\n",
        "full_y = pd.concat([y_train, y_val], axis=0)\n",
        "\n",
        "full_train_fe, full_train_columns = prepare_features(full_train_df, categorical_cols, is_train=True)\n",
        "\n",
        "# Set kategori jika perlu\n",
        "for col in categorical_cols:\n",
        "    if col in full_train_fe.columns:\n",
        "        full_train_fe[col] = full_train_fe[col].astype('category')\n",
        "\n",
        "# Training final model\n",
        "dtrain_full = lgb.Dataset(full_train_fe, label=full_y)\n",
        "final_model = lgb.train(\n",
        "    final_params,\n",
        "    dtrain_full,\n",
        "    num_boost_round=1000,\n",
        ")\n",
        "\n",
        "ml.save_model(final_model, \"lgbm_model_2.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703fe37d",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1633c5fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models\\lgbm_model_2.pkl\n"
          ]
        }
      ],
      "source": [
        "model = ml.load_model(\"lgbm_model_2.pkl\")\n",
        "preds = model.predict(test_fe)\n",
        "pred_labels = (preds > 0.5).astype(int)\n",
        "\n",
        "submission['diagnosed_diabetes'] = preds\n",
        "submission.to_csv('output/submission_2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
