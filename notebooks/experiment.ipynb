{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Experimentation\n",
        "# Only run once\n",
        "import sys\n",
        "import os\n",
        "\n",
        "project_root = os.path.abspath('../')\n",
        "os.chdir(project_root)\n",
        "\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22302dea",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8b3a8d7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from utils.load_data import DataLoader\n",
        "from utils.preprocess import Preprocessor, FeatureEngineering\n",
        "from utils.evaluate_model import Evaluator\n",
        "from utils.model import ModelLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "import mlflow\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "dl = DataLoader()\n",
        "pre = Preprocessor()\n",
        "fe = FeatureEngineering()\n",
        "evaluator = Evaluator()\n",
        "ml = ModelLoader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a73fa3e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"file:///c:/Users/Axioo/Documents/Fahmi/ai/ml/diabetes-prediction-challenge/logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "be57e019",
      "metadata": {},
      "outputs": [],
      "source": [
        "train = dl.load_data('train_clean.csv', 'data/processed')\n",
        "test = dl.load_data('test_clean.csv', 'data/processed')\n",
        "submission = dl.load_data('sample_submission.csv', 'data/raw')\n",
        "\n",
        "train_df = train.copy()\n",
        "test_df = test.copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = dl.split_data(train_df, id_column='id', target_column='diagnosed_diabetes', test_size=0.2, random_state=42, stratify='diagnosed_diabetes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47d745c",
      "metadata": {},
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8cab7b32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function\n",
        "def create_features(df):\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # 1. BMI Categories\n",
        "    df_new['bmi_category'] = pd.cut(df_new['bmi'], bins=[0, 18.5, 25, 30, 100],labels=['underweight', 'normal', 'overweight', 'obese'])\n",
        "    \n",
        "    # 2. Age Groups\n",
        "    df_new['age_group'] = pd.cut(df_new['age'], bins=[0, 30, 45, 60, 100],labels=['young', 'middle', 'senior', 'elderly'])\n",
        "    \n",
        "    # 3. Blood Pressure Categories (using systolic_bp)\n",
        "    df_new['bp_category'] = pd.cut(df_new['systolic_bp'],bins=[0, 120, 130, 140, 200],labels=['normal', 'elevated', 'high', 'very_high'])\n",
        "    \n",
        "    # 4. Cardiovascular Risk Score\n",
        "    df_new['cardio_risk_score'] = (\n",
        "        (df_new['systolic_bp'] / 140) * 0.3 +\n",
        "        (df_new['diastolic_bp'] / 90) * 0.3 +\n",
        "        (df_new['heart_rate'] / 100) * 0.2 +\n",
        "        df_new['cardiovascular_history'] * 0.2\n",
        "    )\n",
        "    \n",
        "    # 5. Cholesterol Risk Score\n",
        "    df_new['cholesterol_risk'] = (\n",
        "        (df_new['cholesterol_total'] / 240) * 0.4 +\n",
        "        (df_new['ldl_cholesterol'] / 160) * 0.4 +\n",
        "        (1 - df_new['hdl_cholesterol'] / 60) * 0.2\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    # 6. Lifestyle Risk Score\n",
        "    df_new['lifestyle_risk'] = (\n",
        "        (df_new['alcohol_consumption_per_week'] / 7) * 0.2 +\n",
        "        (1 - df_new['physical_activity_minutes_per_week'] / 300) * 0.3 +\n",
        "        (df_new['screen_time_hours_per_day'] / 10) * 0.2 +\n",
        "        (1 - df_new['sleep_hours_per_day'] / 8) * 0.3\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    # 7. Diet-Activity Balance\n",
        "    df_new['diet_activity_balance'] = (\n",
        "        df_new['diet_score'] * df_new['physical_activity_minutes_per_week'] / 1000\n",
        "    )\n",
        "    \n",
        "    # 8. Metabolic Health Index\n",
        "    df_new['metabolic_index'] = (\n",
        "        (df_new['triglycerides'] / 150) * 0.3 +\n",
        "        (df_new['bmi'] / 30) * 0.4 +\n",
        "        (df_new['waist_to_hip_ratio'] / 0.95) * 0.3\n",
        "    )\n",
        "    \n",
        "    # 9. Overall Health Risk\n",
        "    df_new['overall_health_risk'] = (\n",
        "        df_new['cardio_risk_score'] * 0.25 +\n",
        "        df_new['cholesterol_risk'] * 0.25 +\n",
        "        df_new['lifestyle_risk'] * 0.2 +\n",
        "        df_new['metabolic_index'] * 0.3\n",
        "    )\n",
        "    \n",
        "    # 10. Interaction Features\n",
        "    df_new['age_bmi'] = df_new['age'] * df_new['bmi']\n",
        "    df_new['age_bp'] = df_new['age'] * df_new['systolic_bp']\n",
        "    df_new['bmi_cholesterol'] = df_new['bmi'] * df_new['cholesterol_total']\n",
        "    df_new['family_history_age'] = df_new['family_history_diabetes'] * df_new['age']\n",
        "    \n",
        "    # 11. Log Transform for Skewed Features\n",
        "    df_new['log_physical_activity'] = np.log1p(df_new['physical_activity_minutes_per_week'])\n",
        "    df_new['log_triglycerides'] = np.log1p(df_new['triglycerides'])\n",
        "    \n",
        "    # 12. Squared Features for Non-linear Relationships\n",
        "    df_new['age_squared'] = df_new['age'] ** 2\n",
        "    df_new['bmi_squared'] = df_new['bmi'] ** 2\n",
        "    \n",
        "    # 13. Binary Risk Flags\n",
        "    df_new['high_bp_flag'] = (df_new['systolic_bp'] > 140).astype(int)\n",
        "    df_new['high_cholesterol_flag'] = (df_new['cholesterol_total'] > 240).astype(int)\n",
        "    df_new['obese_flag'] = (df_new['bmi'] > 30).astype(int)\n",
        "    df_new['sedentary_flag'] = (df_new['physical_activity_minutes_per_week'] < 150).astype(int)\n",
        "    \n",
        "    # 14. Combined Family and Medical History\n",
        "    df_new['medical_history_score'] = (\n",
        "        df_new['family_history_diabetes'] + \n",
        "        df_new['hypertension_history'] + \n",
        "        df_new['cardiovascular_history']\n",
        "    )\n",
        "    \n",
        "    return df_new\n",
        "\n",
        "def prepare_features(df, categorical_cols, is_train=True, train_columns=None):\n",
        "    df_fe = create_features(df)\n",
        "    cat_cols = categorical_cols + ['bmi_category', 'age_group', 'bp_category']\n",
        "    df_encoded = pd.get_dummies(df_fe, columns=cat_cols, drop_first=True)\n",
        "    \n",
        "    if is_train:\n",
        "        return df_encoded, df_encoded.columns.tolist()\n",
        "    else:\n",
        "        if train_columns is None:\n",
        "            raise ValueError(\"train_columns must be provided when is_train=False\")\n",
        "        \n",
        "        for col in train_columns:\n",
        "            if col not in df_encoded.columns:\n",
        "                df_encoded[col] = 0\n",
        "        \n",
        "        df_encoded = df_encoded[train_columns]\n",
        "        \n",
        "        return df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8beca1bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execution\n",
        "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "X_train_fe, train_columns = prepare_features(X_train, categorical_cols, is_train=True)\n",
        "X_val_fe = prepare_features(X_val, categorical_cols, is_train=False, train_columns=train_columns)\n",
        "test_fe = prepare_features(test_df, categorical_cols, is_train=False, train_columns=train_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65898c41",
      "metadata": {},
      "source": [
        "# Tuning hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0a48f011",
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "for col in X_train_fe.select_dtypes(include='uint8').columns:\n",
        "    X_train_fe[col] = X_train_fe[col].astype('category')\n",
        "    X_val_fe[col] = X_val_fe[col].astype('category')\n",
        "        \n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"random_state\": 42,\n",
        "        \"verbosity\": -1,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 50, 400),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 20, log=True),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-3, 100.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-3, 100.0, log=True),\n",
        "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.0, 1.0),\n",
        "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 10.0),\n",
        "    }\n",
        "    \n",
        "    dtrain = lgb.Dataset(X_train_fe, label=y_train)\n",
        "    dval = lgb.Dataset(X_val_fe, label=y_val, reference=dtrain)\n",
        "    \n",
        "    with mlflow.start_run(nested=True):\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dval],\n",
        "            num_boost_round=700,\n",
        "            callbacks=[lgb.early_stopping(100)]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val_fe, num_iteration=model.best_iteration)\n",
        "        auc = roc_auc_score(y_val, preds)\n",
        "\n",
        "        mlflow.log_params(params)\n",
        "        mlflow.log_metric(\"val_auc\", auc)\n",
        "        mlflow.log_metric(\"best_iteration\", model.best_iteration)\n",
        "        mlflow.log_metric(\"train_auc\", roc_auc_score(y_train, model.predict(X_train_fe, num_iteration=model.best_iteration)))\n",
        "        \n",
        "        signature = mlflow.models.infer_signature(X_train_fe, preds)\n",
        "        mlflow.lightgbm.log_model(\n",
        "            model, \n",
        "            name=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=None\n",
        "        )\n",
        "\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ab17d9be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[33]\tvalid_0's binary_logloss: 0.657931\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[106]\tvalid_0's binary_logloss: 0.638175\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's binary_logloss: 0.650424\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's binary_logloss: 0.659888\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's binary_logloss: 0.658395\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.659472\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[109]\tvalid_0's binary_logloss: 0.64601\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[39]\tvalid_0's binary_logloss: 0.645092\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's binary_logloss: 0.661531\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's binary_logloss: 0.660587\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's binary_logloss: 0.653589\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.594015\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.587032\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[696]\tvalid_0's binary_logloss: 0.589212\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[670]\tvalid_0's binary_logloss: 0.587791\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's binary_logloss: 0.660758\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[12]\tvalid_0's binary_logloss: 0.655062\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[608]\tvalid_0's binary_logloss: 0.585786\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's binary_logloss: 0.651225\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[40]\tvalid_0's binary_logloss: 0.628831\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.65973\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.587775\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.615225\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\tvalid_0's binary_logloss: 0.643494\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[686]\tvalid_0's binary_logloss: 0.585434\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.651535\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[145]\tvalid_0's binary_logloss: 0.624359\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's binary_logloss: 0.648182\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's binary_logloss: 0.655139\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[692]\tvalid_0's binary_logloss: 0.608305\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[11]\tvalid_0's binary_logloss: 0.657506\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.604693\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.638952\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[687]\tvalid_0's binary_logloss: 0.612792\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[672]\tvalid_0's binary_logloss: 0.611205\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's binary_logloss: 0.647311\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's binary_logloss: 0.641148\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.650097\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.66035\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's binary_logloss: 0.659905\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[682]\tvalid_0's binary_logloss: 0.610636\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.603327\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[168]\tvalid_0's binary_logloss: 0.638422\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's binary_logloss: 0.630933\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[22]\tvalid_0's binary_logloss: 0.640562\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.603504\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.604614\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's binary_logloss: 0.654839\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.647796\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[33]\tvalid_0's binary_logloss: 0.636916\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's binary_logloss: 0.658649\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[672]\tvalid_0's binary_logloss: 0.601276\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.595633\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's binary_logloss: 0.627371\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[696]\tvalid_0's binary_logloss: 0.584588\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.59762\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.656031\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[161]\tvalid_0's binary_logloss: 0.623355\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's binary_logloss: 0.6609\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.587319\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[622]\tvalid_0's binary_logloss: 0.593024\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.584591\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.607372\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[66]\tvalid_0's binary_logloss: 0.632167\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.60693\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[698]\tvalid_0's binary_logloss: 0.589837\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[672]\tvalid_0's binary_logloss: 0.592847\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.650889\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's binary_logloss: 0.644231\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's binary_logloss: 0.634122\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.59337\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.593678\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's binary_logloss: 0.590672\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.587983\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.593309\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[671]\tvalid_0's binary_logloss: 0.59061\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's binary_logloss: 0.587606\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[676]\tvalid_0's binary_logloss: 0.590229\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's binary_logloss: 0.63432\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's binary_logloss: 0.643285\n",
            "Best AUC: 0.725827\n",
            "Best params:\n",
            "  learning_rate: 0.03436878560983435\n",
            "  num_leaves: 90\n",
            "  max_depth: 14\n",
            "  min_child_samples: 60\n",
            "  min_child_weight: 0.20924809433602157\n",
            "  feature_fraction: 0.6050995387290706\n",
            "  bagging_fraction: 0.5873549599767341\n",
            "  bagging_freq: 4\n",
            "  lambda_l1: 1.3078712844062566\n",
            "  lambda_l2: 0.9011118029399013\n",
            "  min_gain_to_split: 0.3524522834854504\n",
            "  scale_pos_weight: 1.3671284001409418\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_experiment(\"lgbm\")\n",
        "with mlflow.start_run(run_name=\"lgbm_hyperparameter_tuning_3\"):\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=80)\n",
        "    \n",
        "    best_params = study.best_params\n",
        "    best_value = study.best_value\n",
        "    \n",
        "    print(f\"Best AUC: {study.best_value:.6f}\")\n",
        "    print(\"Best params:\")\n",
        "    for k, v in study.best_params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    \n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"best_val_auc\", study.best_value)\n",
        "    mlflow.log_metric(\"n_trials_completed\", len(study.trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7027b2bd",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f71c3ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to models\\lgbm_model_3.pkl\n"
          ]
        }
      ],
      "source": [
        "final_params = best_params.copy()\n",
        "final_params.update({\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"random_state\": 42,\n",
        "    \"verbosity\": -1,\n",
        "})\n",
        "\n",
        "full_train_df = pd.concat([X_train, X_val], axis=0)\n",
        "full_y = pd.concat([y_train, y_val], axis=0)\n",
        "\n",
        "full_train_fe, full_train_columns = prepare_features(full_train_df, categorical_cols, is_train=True)\n",
        "\n",
        "# Set kategori jika perlu\n",
        "for col in categorical_cols:\n",
        "    if col in full_train_fe.columns:\n",
        "        full_train_fe[col] = full_train_fe[col].astype('category')\n",
        "\n",
        "# Training final model\n",
        "dtrain_full = lgb.Dataset(full_train_fe, label=full_y)\n",
        "final_model = lgb.train(\n",
        "    final_params,\n",
        "    dtrain_full,\n",
        "    num_boost_round=1000,\n",
        ")\n",
        "\n",
        "ml.save_model(final_model, \"lgbm_model_3.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703fe37d",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1633c5fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models\\lgbm_model_3.pkl\n"
          ]
        }
      ],
      "source": [
        "model = ml.load_model(\"lgbm_model_3.pkl\")\n",
        "preds = model.predict(test_fe)\n",
        "pred_labels = (preds > 0.5).astype(int)\n",
        "\n",
        "submission['diagnosed_diabetes'] = preds\n",
        "submission.to_csv('output/submission_3.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
